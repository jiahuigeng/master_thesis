%%
%% i6-style slides
%% customized for kim
%%

\documentclass[11pt, a4paper, landscape]{article}
\usepackage[userlastpage,triangle,utf-8,noblackslide]{NeyDreuwSlides_Oct08}
\usepackage{snapshot}
\usepackage{mathtools}
\usepackage{amssymb,amsmath,amsthm,enumitem}
\usepackage[caption]{subfig}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{bm}


% slides tunning
% \setlength{\footskip}{0pt} % default ist +30pt - negative Werte nicht moeglich
% \setlength{\headsep}{-10pt} % default ist +25pt - negative Werte moeglich
% \addtolength{\topmargin}{-5mm} % bewegt den gesamten Inhalt der Seite nach oben
% \setlength{\textheight}{170mm} % wenn moeglich nicht veraendern, beinflusst den Inhalt der Seiten und somit auch die Anzahl der Seiten!

% math operators and
% symbols %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\nat}{\ensuremath{\rm I\!N}\xspace}
\newcommand*{\rel}{\ensuremath{\rm I\!R}\xspace}
\newcommand*{\argmin}{\ensuremath{\operatornamewithlimits{argmin}}\xspace}
\newcommand*{\argmax}{\ensuremath{\operatornamewithlimits{argmax}}\xspace}
\newcommand*{\congmod}{\ensuremath{\operatornamewithlimits{\cong}}\xspace}
\newcommand*{\invers}{\ensuremath{\frac{1}}\xspace}
\newcommand*{\ra}{\ensuremath{\Rightarrow}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom packages
\usepackage{fancyvrb} %%% fancy verbatim to enable coloring within verbatim environments
\usepackage{bbding}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{tikz}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\renewcommand{\arraystretch}{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% main title of the work (used for \TitlePage)
\renewcommand*{\title}{Unsupervised Learning of Cross-lingual Word Embedding and Its Application to Machine Translation}
% short title (used for \lfoot)
\renewcommand*{\titleshort}{Unsupervised and Cross-lingual Embedding and Application in MT}
% (used for \TitlePage)
\renewcommand*{\occasion}{Master Thesis Mid-term Talk\\}
% short occasion title (used for \rfoot)
\renewcommand*{\occasionshort}{}
% default is \today (used for \TitlePage and \rfoot)
\renewcommand*{\date}{\today}
% all the authors of the work, can be long (used for \TitlePage)

\renewcommand*{\author}{Jiahui Geng}

% all the authors of the work, should be short (used for \lfoot)
\renewcommand*{\authorshort}{J. Geng:\xspace}
% all email address(es) of the authors (used for \TitlePage)
\renewcommand*{\email}{\url{jiahui.geng@rwth-aachen.de}}
% the author(s) who presented the work (used for \TitlePage)
\renewcommand*{\mainauthor}{Jiahui Geng}
% presenter mail address(es) (used for \FinalPage)
\renewcommand*{\mainauthoremail}{\url{jgeng@cs.rwth-aachen.de}}
% web address (used for \TitlePage _and_ \FinalPage)
\renewcommand*{\www}{http://www.hltpr.rwth-aachen.de/}
% keywords, can be used for PDF summary
\newcommand*{\keywords}{}

% will be set into the PDF document summary
\hypersetup{ pdftitle={\title}, pdfsubject={\occasion},
	pdfauthor={\author}, pdfkeywords={\keywords}, pdfpageduration = 2,
	pdfpagetransition = {Box /M /O /D 1}, }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listfiles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	%\setlength{\abovedisplayskip}{5pt}
	%\setlength{\belowdisplayskip}{3pt}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\TitlePage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
	\NewPage
	
	\headline {Outline}
	
	\vfill
	\begin{description}
		\item Introduction
		\item Literature
		\item Cross-lingual word embedding
		\begin{itemize}
			\item Monolingual word embedding
			\item Linear mapping between embedding spaces 
			\begin{itemize}
				\item Supervised learning
				\item Unsupervised learning
			\end{itemize}
		\end{itemize}
		\item Sentence Translation
		\begin{itemize}
			\item Context-aware beam search
			\item Denoising autoencoder
		\end{itemize}
		\item Experiments

		\item Outlook
	\end{description}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Introduction}
	
	\vfill
	\begin{description}
		\item Motivation
		\begin{itemize}
			\item Building a machine translation system requires lots of bilingual data
			\item Cross-lingual word embedding offers elegant word matches between languages
			\item Unsupervised MT relies on back-translation which needs a long training time
		\end{itemize}
		\item Goals
		\begin{itemize}
			\item Study training details of cross-lingual word embedding
			\item Build a good unsupervised MT efficiently: combine with other models
			\item Improve the unsupervised learning algorithm for cross lingual word embedding
		\end{itemize}
		
	\end{description}
	\vfill
	

	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised cross-lingual embedding
	\begin{itemize}
		\item 	\cite{artetxe2017learning} Learning bilingual word embeddings with (almost) no bilingual data
		\begin{itemize}
			\item A self-learning framework combining embedding mapping and dictionary induction techniques, needs small dataset to start
		\end{itemize}	    
		\item \cite{hoshen2018iterative} An Iterative Closest Point Method for Unsupervised Word Translation	
		\item 	\cite{conneau2017word} Word translation without parallel data
		\begin{itemize}
			\item Implementation of GANs: discriminator trained to distinguish between two distributions while generator fools discriminator when learning mapping
		\end{itemize}	    
		\begin{itemize}
			\item Iterative closest point method for embedding mapping learning, without neural network but more interpretable	
		\end{itemize}	    
	\end{itemize}
	
	
	
	
	
	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised machine translation
	\begin{itemize}
		\item 	\cite{artetxe2017unsupervised} Unsupervised Neural Machine Translation
		\item \cite{lample2017unsupervised} Unsupervised Machine Translation Using Monolingual Corpora Only
		\begin{itemize} 
			\item Seq2seq model with shared encoder and decoder for both languages, also with denoising autoencoder and back-translation
		\end{itemize}
		
		\item \cite{artetxe2017unsupervised} Phrase-Based \& Neural Unsupervised Machine Translation
		\begin{itemize}
			\item Simplifying the architecture and loss function, still following the above mentioned principles and propose a phrase-based SMT with back-translation
		\end{itemize}	
		

	\end{itemize}
	
	\vfill


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Word Translation}
	\vfill
	\begin{minipage}[b]{.6\linewidth}
		\centering
		\begin{itemize}
			\item Learn monolingual embedding separately
			\begin{itemize}
				\item Skip-gram model \\ \texttt{fasttext} \cite{joulin2016fasttext} 
			\end{itemize}
			\item Learn linear mapping between embedding spaces	
			\begin{itemize}
				\item Supervised learning
				\begin{itemize}
					\item Procrustes analysis
				\end{itemize}
				\item Unsupervised learning
				\begin{itemize}
					\item Adversarial learning
					\item Iterative refinement
				\end{itemize}
			\end{itemize}
			\item Bidirectional dictionary induction
			\begin{itemize}
				\item CSLS retrieval 
			\end{itemize}
			
		\end{itemize}		
	\end{minipage}%
	\begin{minipage}[t]{.4\linewidth}
		\centering 
		\includegraphics[width=7cm]{cross-embed}
	\end{minipage}
	\vfill



	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Monolingual Embedding}
	\vfill
	\texttt{Fasttext}
	\begin{itemize}
		\item Essentially an extension of skip-gram/CBOW model
		\item Treat each word as composed of character ${n}$-gram
		\item Learn the internal structure of words\\
	\end{itemize}
	Problem
	\begin{itemize}
		\item Not accurate for rare words\ (usually name entities) 
	\end{itemize}
	
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill
	
	Definition
	\begin{itemize}
		\item Word embedding of multiple languages in a joint embedding space		\\
	\end{itemize}
	Roles in unsupervised neural machine translation 
	\begin{itemize}
		\item Shared latent representations
		\begin{itemize}
			\item Shared encoder for producing a language independent representation
			\item Back-translation for further improvement
		\end{itemize}
		\item This work
		\begin{itemize}
			\item Formulate a straightforward way to combine
			a language model with cross-lingual
			word similarities
		\end{itemize} 
		
		
	\end{itemize}
	\vfill
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill
	
	Training Methods
	\begin{itemize}
		\item Mapping-based approaches (this work)
		\begin{itemize}
			\item Train word embeddings separately then learn mapping with bilingual dictionaries
		\end{itemize}
		\item Pseudo-multi-lingual corpora-based approaches
		\begin{itemize}
			\item Use monolingual word embedding methods on mixed corpus of multiple languages 
		\end{itemize}
		\item Joint methods
		\begin{itemize}
			\item Minimize the monolingual losses with the cross-lingual regularization term
		\end{itemize}
	\end{itemize}
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Supervised Learning}
	\vfill
	Assume we have
	\begin{itemize}
		\item Word embedding \\
		trained  independently for each language on monolingual corpora
		\item Bilingual dictionary \\
		a known dictionary with pairs of words ${ \{ \bm{f}, \bm{e} \} }$ size s
	\end{itemize}
	
	
	
	Learn a linear mapping ${W \in \mathbb{R}^{d \times d}}$ such that
	\[W^*= \argmin\limits_{W \in \mathbb{R}^{d \times d}} {\sum_{i=1}^{s} {\lVert Wf_i -e_i \rVert}} \]
	\begin{itemize}
		\item ${d:}$ Dimension of embedding
		\item $f_i, e_i \in \mathbb{R}^d $: the embedding pair of corresponding word pair in the dictionary
	\end{itemize}
	\vfill
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Procrustes Analysis}
	\vfill
	Constrain ${W}$ to be an orthogonal matrix
	\begin{itemize}
		\item Enforce monolingual invariance
		
		\item Simplify the problem as the Procrustes problem
		
		\begin{itemize}
			\item A closed-form solution obtained from SVD
			\item $E$, $F \in \mathbb{R}^{d*s}$ denotes embedding projection of word pairs ${\{e,f\}}$
		\end{itemize}
		
		\[ W^* = \argmin\limits_{W \in \mathbb{R}^{d \times d}} \lVert WF-E\lVert  =  UV^T\]
		\[ U\varSigma V^T =  \textrm{SVD}(EF^T)\]
		
		\item Can be efficiently computed in linear time  w.r.t. seed dictionary size $s$
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Unsupervised Word Embedding Mapping}	
	\vfill
	Problem
	\begin{itemize}
		\item Large dictionary not readily available for many language pairs\\
	\end{itemize}
	
	
	Methods
	\begin{itemize}
		\item 	Learn bilingual embeddings without any bilingual evidence (this work)
		\begin{itemize}
			\item Adversarial training			
		\end{itemize}
		\item Design the seed dictionary
		\begin{itemize}
			\item Shared words, digits and cognates
			\item Design heuristics to
			build the seed dictionary
		\end{itemize}
		
	\end{itemize}
	\vfill
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Adversarial Training}
	\vfill
	Model
	\begin{itemize}
		\item  ${\mathcal{F} = \left\{ f_1, \ldots f_{V_f} \right\} }$ and  ${\mathcal{E} = \left\{ e_1, \ldots e_{V_e} \right\} }$: set of embeddings, not parallel
		\item Discriminator is trained to discriminate ${W{f_n}}$ and ${e_n}$ with ${f_n}$, ${e_n}$ randomly sampled from ${ \mathcal{F},  \mathcal{E} }$ 
		\item Generator ${W}$ is trained to prevent the discriminator from making accurate prediction
		
	\end{itemize}
	
	
	
	Discriminator loss
	\[ \mathcal{L}_D(\theta_D|W) = -\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta_D}(source=1| W f_i) - \frac{1}{m}\sum_{i=1}^{m}\log P_{\theta_D}(source=0| e_i)\]
	
	
	Generator  loss 
	\[ \mathcal{L}_D(W|\theta_D) = -\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta_D}(source=0| W f_i) - \frac{1}{m}\sum_{i=1}^{m}\log P_{\theta_D}(source=1| e_i)\]
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\NewPage
	\headline{Iterative Refinement}
	\vfill
	Self-learning framework
	\begin{enumerate}
		\item Dictionary is important to train the cross-lingual embedding
		\item Start from a initial dictionary, use such dictionary as input to learn cross-lingual mapping
		\item Assume the dictionary inducted from the learned mapping is better and can provide better mapping further
		\item Design a convergence criterion, if not satisfied, keep training
	\end{enumerate}



	
	\vfill


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Dictionary Induction}

	\vfill
	Cross-domain Similarity Local Scaling (CSLS)
	\begin{itemize}
		\item Nearest neighbour search suffers from the hubness problem
		\begin{itemize}
			\item Points tending to be nearest neighbors of many points in high-dimensional spaces
		\end{itemize}
		\item Penalize the similarity score of hubs
			\begin{itemize}
				\item $N_T(Wf):$ target neighbours for mapped source embedding
				\item $r_T(W f):$ penalty for hubness
			\end{itemize}
	\end{itemize}
	\[r_T(W f) = \frac{1}{K} \sum_{e \in N_T(Wf)}\textrm{cos}(Wf, e) \]
	\[ \textrm{CSLS}(Wf, e) = 2\ \textrm{cos}(Wf, e)-r_{T}(Wf)-r_{S}(e)\]
	Bidirection dictionary induction

	\begin{itemize}
		\item Unidirectional dictionary might lead to local optima
		\item Include only the mutual nearest neighbors
		\item Select more probable candidates as pairs   
	\end{itemize}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Sentence Translation}
	\vfill
	Context-aware Beam Search
	\begin{itemize}
		\item Language model\\
	\end{itemize}
	Denoising Autoencoder
	\begin{itemize}
		\item Insertion
		\item Deletion
		\item Reordering
	\end{itemize}	
	\vfill
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	
	\headline{Context-aware beam search}
	\vfill
	
	\begin{itemize}
		\item Given a history $h$ of target word before $e$, the score of $e$ to be the translation of $f$:
		\[ L(e;f,h)=\lambda_{emb}q(f,e) + \lambda_{LM}p(e|h)\]
		\item Lexicon score $q(f,e) \in [0,1] $ defined as:
		\[ q(f,e)= \frac{d(f,e)+1}{2}\]
		\item $d(f,e)\in [-1,1]$ cosine similarity between $f$ and $e$
		\item In experiments, such lexicon score works better than others, e.g. sigmoid or softmax
	\end{itemize}	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Denoising}	
\vfill
\begin{itemize}
	\item Model such  $c(e_1^I)$ by injecting artificial noise into clean sentences $e_1^I$
	\item Training criterion:
	\[ L = E_{e_1^I \in E}[-log(e_1^I|C(e_1^I))]\]
	\item In Seq2Seq training, $e_1^I$ as label, $c(e_1^I)$  as input 
	\item Artificial noise:
	\begin{itemize}
		\item insertion, deletion, reordering
	\end{itemize} 
	
\end{itemize}
\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Insertion}	
\vfill	
Insertion
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item Word-by-word translation always outputs a target word for every position
		\item Some common words are considered as redundant ones
	\end{itemize}
	\item Method
	\begin{itemize}
		\item For each position in a sentence, insert a frequent word according from set ${v_{ins}}$ to a probability distribution ${p_{ins}}$
		\item Denoising network learns to delete the word when translating
	\end{itemize}
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{insertion}
	\end{center}\vspace{0.5em}	
\end{itemize}

\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Deletion}
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item In contrary case: some words are not related to any source word
	\end{itemize}
	
	\item Realization
	\begin{itemize}
		\item For each position in a sentence, delete the word according to a probability distribution ${p_{del}}$ as input
		\item  Denoising network learns to add some potential words when translating
	\end{itemize}	
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{deletion}
	\end{center}\vspace{0.5em}	
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Reordering}	
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item Generated words are not in a correct sequence of the target language
	\end{itemize}
	\item Method
	\begin{itemize}
		\item For each position of a sentence, swap the words within a limited distance $d_{per}$ as input
		\item Denoising network learns reordering information when translating
	\end{itemize}
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{denoising}
	\end{center}\vspace{0.5em}
\end{itemize}	



	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Experiment Settings }	
	\vfill
	\begin{itemize}
	\item Word embedding and LM learned on 100M sentences from \texttt{wmt 2014-2017}
	\item BLEU evaluated on German$\leftrightarrow$English \texttt{newstest2016}
	\item {Word accuracy evaluated on  dictionaries released by Facebook}
		\begin{itemize}
			\item Dictionary built with internal translation tool
			\item Each word has 1-4 word translation(s)
			\item Top-1 accuracy: if top 1 candidate in the dictionary			
		\end{itemize}
	\item Context-aware beam search: Lexicon candidates: 100 / beam width 10

	 
	\end{itemize}
	\vfill
%%%%%%%%%%%%%%%
\NewPage
\headline{Experiments}
\vfill

\begin{table}
	\setcounter{table}{1}
	\caption{Translation results on German$\leftrightarrow$English \texttt{newstest2016} and French$\leftrightarrow$English \texttt{newstest2014}.}
	\centering
	\begin{tabular}{>{\bfseries}l>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c}
		\toprule
		&  & de-en & en-de & fr-en & en-fr\\
		System & OOV  & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%]\\
		\midrule
		Word-by-Word & None  & 11.1 & 6.7 & 10.6 & 7.8\\
		\midrule
		+ LM  & LM  & 12.9 & 8.9 & 12.7 & 10.0\\
		&  Copy	& 14.5 & 9.9 & 13.6 & 10.9\\
		\midrule
		\hspace{10pt}+ Denoising (RNN) &  & 16.2 & 10.6 & 15.8 & 13.3 \\
		\hspace{10pt}+ Denoising (Transformer) & & \leavevmode\color{blue}{17.2} & \leavevmode\color{blue}{11.0} & \leavevmode\color{blue}{16.5} & \leavevmode\color{blue}13.9 \\
		\midrule
		\cite{lample2017unsupervised} & & 13.3 & 9.6 & 14.3 & 15.1\\
		\cite{artetxe2017unsupervised} & & - & - & 15.6 & 15.1\\
		\bottomrule
	\end{tabular}
	
	\label{tab:results}
\end{table}	
\vfill	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Ablation studies}	
	\vfill
	\begin{itemize}
		\item Different sizes of training corpora
		\item Different vocabularies: BPE and word
		\item Different vocabulary sizes for cross-lingual training
		\item Different denoising parameters
		\item Phrase embedding
		\item Vocabulary cut-off
		
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Different Training Corpora }	
	\vfill

	\begin{table}[!h]
		\centering
		\caption {Word-by-word translation from German to English}
		\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c}
			\hline
			&\textsc{Accuracy} [\%]& \textsc{Bleu} [\%] \\ \hline
			5M & 44.9  & 9.7  \\ \hline
			10M & 51.6 & 10.1 \\ \hline
			50M & 59.4 & 10.8 \\ \hline
			100M &\leavevmode\color{blue}61.2 & \leavevmode\color{blue}11.2 \\ \hline
		\end{tabular}
	
	\end{table}
	
	\begin{itemize}
		\item Larger corpus improves the word translation accuracy
		\item Also improves the word-by-word translation
	\end{itemize}
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Different Embeddings and Traning Vocabulary Size}

	
	\begin{table}[!ht]
		\centering
		\scalebox{0.80}{
		\begin{tabular}{>{\bfseries}l>{\bfseries}c>{\bfseries}c}
			\toprule
			\multicolumn{2}{c}{\textbf{Vocabulary}} & \textsc{Bleu} [\%] \\
			\midrule
			& Merges \\
			\cmidrule{2-2}
			\multirow{3}{*}{BPE} & 
			20k & 10.4 \\
			& 50k & 12.5 \\
			& 100k & \leavevmode\color{blue}13.0 \\
			\midrule
			& Cross-lingual training \\
			\cmidrule{2-2}
			\multirow{4}{*}{Word} & 20k & 14.4\\
			& 50k & 14.4\\
			& 100k & \leavevmode\color{blue}14.5\\
			& 200k & 14.4\\
			\bottomrule
		\end{tabular}
			}
		

	\end{table}

	\begin{itemize}
		\item Word-by-word translation with language model
		\item Word embedding performs better than BPE embedding
		\item Embedding trained on 20k similar to 200k $\Rightarrow$ Frequent words matter
	\end{itemize}	
		\vfill





	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Denoising Experiments}
		\vfill

		\begin{table}[!h]
			\centering
			\scalebox{0.80}{
			\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}r>{\bfseries}c}
				\toprule
				$d_\text{per}$ & $p_\text{del}$ & $p_\text{ins}$ & $p_\text{ins}$ & \textsc{Bleu} [\%] \\
				\midrule
				2 & & & & 14.7\\
				3 & & & & \leavevmode\color{blue}{14.9}\\
				5 & & & & 14.9\\
				\midrule
				\multirow{2}{*}{3} & 0.1 & &  & \leavevmode\color{blue}{15.7} \\
				& 0.3 & & & 15.1 \\
				\midrule
				\multirow{4}{*}{3} & \multirow{4}{*}{0.1} & \multirow{4}{*}{0.1} & 10 & 16.8 \\
				& & & 50 & \leavevmode\color{blue}{17.2} \\
				& & & 500 & 16.8 \\
				& & & 5000 & 16.5\\
				\bottomrule
			\end{tabular}
			}
			\setcounter{table}{1}
			\label{tab:denoising}
		\end{table}

	\begin{itemize}
		\item Each artificial noise improves the translation performance
	\end{itemize}
		\vfill	
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%in a small range%%%%%%%%%%%%%%%%%
\NewPage
\headline{Phrase Embedding}
Motivation
\begin{itemize}
	\item Many phrases have a meaning that is not a simple composition of the meaning of its individual words\\
\end{itemize}
Phrase detection
\begin{itemize}
	\item Phrases formed based on the unigram and bigram counts:\\
	\cite{mikolov2013distributed} 
	\begin{itemize}
		\item Tune a good threshold value for score
	\end{itemize}
	\[ \textrm{score}(e^\prime, e) = \frac{\textrm{count}(e^\prime, e) - \delta}{\textrm{count}(e^\prime)*\textrm{count}(e)} \]	
	\item Process sentences with most common phrases in training corpus
	\begin{itemize}
		\item Count the most frequent bi-gram phrases: ${\textrm{score}(e^\prime, e) = \textrm{count}(e^\prime, e)}$
		\item Detect phrases as top frequent phrases in the training corpus
	\end{itemize}
	
\end{itemize}	
\NewPage
\headline{Phrase Embeddings}
\vfill
\begin{table}[]
	\centering
	\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c  >{\bfseries}c}
		\hline
		\multicolumn{3}{c}{\multirow{2}{*}{\textbf{Vocabulary}}}                  & No LM & With LM & Denoising \\
		\multicolumn{3}{c}{}                                         &  \textsc{Bleu} [\%]  &  \textsc{Bleu} [\%] & \textsc{Bleu} [\%]   \\ \hline
		Word            & \multicolumn{2}{l}{}              & 11.2 & 14.5  &\leavevmode\color{blue}{ 17.2} \\
		 \hline
		\multirow{3}{*}{\cite{mikolov2013distributed} } & \multirow{3}{*}{threshold} & 100  & 11.1 & 13.7  & 15.6 \\ \cline{3-6} 
		&                            & 500  & 11.0 & 13.7  & 16.2 \\ \cline{3-6} 
		&                            & 2000 & 10.7 & 14.0  &16.5 \\ \hline
		Top frequent              & \multicolumn{1}{l}{\textbf{count}}  & 50k  & \leavevmode\color{blue}12.0 & \leavevmode\color{blue}15.7  & 16.8 \\ \hline
	\end{tabular}
\end{table}
\begin{itemize}
	\item Phrase embeddings helps only for WBW and  $+$LM
\end{itemize}
\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{	Souce and Target Vocanulary Cut-off}

\begin{itemize}
	\item Column: source vocabulary size/ row: target vocabulary size
\end{itemize}


\begin{table}
	\parbox{.5\linewidth}{
		\centering
		\caption{Word embedding vocabulary cut-off}
		\begin{tabular}{>{\bfseries}c >{\bfseries}c >{\bfseries}c >{\bfseries}c } 
			\hline
			\textsc{Bleu} [\%]	& 20k & 50k & 100k \\
			\hline
			50k &	11.1  & \leavevmode\color{blue}11.3 & 11.2  \\ 
			\hline
			100k&	11.2  & 11.2 & 11.1 \\ 			
			\hline
			150k&	10.9 & 10.9 & - \\
			\hline
		\end{tabular}
		
	}
	\hfill
	\parbox{.5\linewidth}{
		\centering
		\caption{Phrase embedding vocabulary cut-off}
		\begin{tabular}{>{\bfseries}c >{\bfseries}c >{\bfseries}c >{\bfseries}c } 
			\hline
			\textsc{Bleu} [\%]	& 50k & 100k & 150k \\
			\hline
			50k &	11.3  & - & -  \\ 
			\hline
			100k&	11.9  & 11.9 & - \\ 			
			\hline
			150k&	\leavevmode\color{blue}12.0 & 11.9 & 11.9 \\
			\hline
			200k & 12.0 & - & - \\
			\hline
		\end{tabular}
		
	}
\end{table}

\begin{itemize}
	\item Vocabulary size effects the translation performance
\end{itemize}



\vfill	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{LM supported Cross-lingual embedding training}
\vfill
Basic idea:
\begin{itemize}
	\item Language model help to select candidates, provide better dictionary
	\item Dictionary from the sentence translation, instead of induction
	\item Training the mapping with SGD instead of Procrustes analysis
\end{itemize}


\vfill

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Conclusions}
	\vfill
	Comprehensive results
	\begin{itemize}
		\item Context-aware beam search with LM helps the lexicon choice
		\item Denoising networks aimed at insertion/deletion/reordering noise works for such problems in a small range of sentences
	\end{itemize}
	Ablation studies
	\begin{itemize}
		\item BPE embeddings performs worse than word embeddings, especially with smaller
		vocabulary size.
		\item Word-by-word translation with cross-lingual
embedding depends highly on the frequent word mappings
		\item Phrase embedding only helps in WBW and Context-aware beam search
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Outlook}
	\vfill
	Goal: Improve the unsupervised learning for cross-lingual embedding

	\begin{itemize}
		\item Accordingly improves unsupervised MT performance
		\item Other applications: transfer learning for low-resource LM \cite{adams2017cross}\\
	\end{itemize}
   
	 Exchange algorithm with LM for inducing initial bilingual dictionary
	 \begin{itemize}
	 \item Adversarial training is not interpretable and relies on random starts
	\item Using LM: strong training signal and less dependence on randomness\\
	 \end{itemize}
	 
	 Non-linear mapping between source and target
	 \begin{itemize}
	 	\item Linear assumption may be too crude
	 	\item Stochastic gradient descent instead of SVD
	 	\item Also applies in supervised case
	 \end{itemize}
	\vfill

	
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\FinalPage
	

	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\bibliographystyle{i6bibliostyle}
	\bibliography{references}
	
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
