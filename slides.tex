%%
%% i6-style slides
%% customized for kim
%%

\documentclass[11pt, a4paper, landscape]{article}
\usepackage[userlastpage,triangle,utf-8,noblackslide]{NeyDreuwSlides_Oct08}
\usepackage{snapshot}
\usepackage{mathtools}
\usepackage{amssymb,amsmath,amsthm,enumitem}
\usepackage[caption]{subfig}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{bm}


% slides tunning
% \setlength{\footskip}{0pt} % default ist +30pt - negative Werte nicht moeglich
% \setlength{\headsep}{-10pt} % default ist +25pt - negative Werte moeglich
% \addtolength{\topmargin}{-5mm} % bewegt den gesamten Inhalt der Seite nach oben
% \setlength{\textheight}{170mm} % wenn moeglich nicht veraendern, beinflusst den Inhalt der Seiten und somit auch die Anzahl der Seiten!

% math operators and
% symbols %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\nat}{\ensuremath{\rm I\!N}\xspace}
\newcommand*{\rel}{\ensuremath{\rm I\!R}\xspace}
\newcommand*{\argmin}{\ensuremath{\operatornamewithlimits{argmin}}\xspace}
\newcommand*{\argmax}{\ensuremath{\operatornamewithlimits{argmax}}\xspace}
\newcommand*{\congmod}{\ensuremath{\operatornamewithlimits{\cong}}\xspace}
\newcommand*{\invers}{\ensuremath{\frac{1}}\xspace}
\newcommand*{\ra}{\ensuremath{\Rightarrow}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom packages
\usepackage{fancyvrb} %%% fancy verbatim to enable coloring within verbatim environments
\usepackage{bbding}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{tikz}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\renewcommand{\arraystretch}{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% main title of the work (used for \TitlePage)
\renewcommand*{\title}{Unsupervised Learning of Cross-lingual Word Embedding and Its Application to Machine Translation}
% short title (used for \lfoot)
\renewcommand*{\titleshort}{Unsupervised and Cross-lingual Embedding and Application in MT}
% (used for \TitlePage)
\renewcommand*{\occasion}{Master Thesis Mid-term Talk\\}
% short occasion title (used for \rfoot)
\renewcommand*{\occasionshort}{}
% default is \today (used for \TitlePage and \rfoot)
\renewcommand*{\date}{\today}
% all the authors of the work, can be long (used for \TitlePage)

\renewcommand*{\author}{Jiahui Geng}

% all the authors of the work, should be short (used for \lfoot)
\renewcommand*{\authorshort}{J. Geng:\xspace}
% all email address(es) of the authors (used for \TitlePage)
\renewcommand*{\email}{\url{jiahui.geng@rwth-aachen.de}}
% the author(s) who presented the work (used for \TitlePage)
\renewcommand*{\mainauthor}{Jiahui Geng}
% presenter mail address(es) (used for \FinalPage)
\renewcommand*{\mainauthoremail}{\url{jgeng@cs.rwth-aachen.de}}
% web address (used for \TitlePage _and_ \FinalPage)
\renewcommand*{\www}{http://www.hltpr.rwth-aachen.de/}
% keywords, can be used for PDF summary
\newcommand*{\keywords}{}

% will be set into the PDF document summary
\hypersetup{ pdftitle={\title}, pdfsubject={\occasion},
	pdfauthor={\author}, pdfkeywords={\keywords}, pdfpageduration = 2,
	pdfpagetransition = {Box /M /O /D 1}, }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listfiles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	%\setlength{\abovedisplayskip}{5pt}
	%\setlength{\belowdisplayskip}{3pt}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\TitlePage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
	\NewPage
	
	\headline {Outline}
	
	\vfill
	\begin{description}
		\item Introduction
		\item Literature
		\item Cross-lingual word embedding
		\begin{itemize}
			\item Supervised learning
			\item Unsupervised learning
		\end{itemize}
		\item Sentence Translation with cross-lingual word embedding
		\begin{itemize}
			\item Context-aware beam search
			\item Denoising autoencoder
		\end{itemize}
		\item Experiments

		\item Outlook
	\end{description}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Introduction}
	
	\vfill
	\begin{description}
		\item Motivation
		\begin{itemize}
			\item Building a machine translation system requires lots of bilingual data
			\item Cross-lingual word embedding offers elegant word matches between languages
			\item Unsupervised MT relies on back-translation which needs a long training time
		\end{itemize}
		\item Goals
		\begin{itemize}
			\item Study training details of cross-lingual word embedding
			\item Build a good unsupervised MT efficiently: combine with other models
			\item Improve the unsupervised learning algorithm for cross lingual word embedding
		\end{itemize}
		
	\end{description}
	\vfill
	

	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised cross-lingual embedding
	\begin{itemize}
		\item 	\cite{conneau2017word} Word translation without parallel data
		\begin{itemize}
		\item Implementation of GANs: discriminator trained to distinguish between two distributions while generator fools discriminator 
		\end{itemize}	
		\item 	\cite{artetxe2017learning} Learning bilingual word embeddings with (almost) no bilingual data
		\begin{itemize}
			\item A self-learning framework combining embedding mapping and dictionary induction techniques, needs seed dictionary to start
		\end{itemize}	    
		\item \cite{hoshen2018iterative} An Iterative Closest Point Method for Unsupervised Word Translation	    
		\begin{itemize}
			\item Iterative closest point method for embedding mapping learning
		\end{itemize}	    
	\end{itemize}
	
	
	
	
	
	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised machine translation
	\begin{itemize}
		\item 	\cite{artetxe2017unsupervised} Unsupervised Neural Machine Translation
		\item \cite{lample2017unsupervised} Unsupervised Machine Translation Using Monolingual Corpora Only
		\begin{itemize} 
			\item Seq2seq model with shared encoder and decoder for both languages, also with denoising autoencoder and back-translation
		\end{itemize}
		
		\item \cite{artetxe2017unsupervised} Phrase-Based \& Neural Unsupervised Machine Translation
		\begin{itemize}
			\item Simplifies the architecture and loss function for unsupervised NMT and propose a phrase-based SMT with back-translation
		\end{itemize}	
		

	\end{itemize}
	
	\vfill



	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill
	
	Definition
	\begin{itemize}
		\item Word embedding of multiple languages in a joint embedding space		
		\item Linear mapping from source embedding to target embedding (this work)
	\end{itemize}
	\begin{figure}
	\begin{minipage}[b]{0.55\textwidth}
	\includegraphics[width=14cm]{crossembedding}
	\caption{cross-lingual word embedding}	
	
	\end{minipage}
	\begin{minipage}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{cross-embed}
		\caption{cross-lingual mapping}

	\end{minipage}	
	\end{figure}

	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill
	Roles in unsupervised neural machine translation 
	\begin{itemize}
		\item Shared latent representations
		\begin{itemize}
			\item Shared encoder for producing a language independent representation
		\end{itemize}	
		\item As word or phrase table for translation\\
	\end{itemize}	
	This work
	\begin{itemize}
	\item Formulate a straightforward way to combine
		a language model with cross-lingual
		word similarities \\
	\end{itemize}
		
	Training Methods
	
	\begin{itemize}
		\item Mapping-based approaches (this work)

		\item Pseudo-multi-lingual corpora-based approaches

		\item Joint methods

	\end{itemize}

	\vfill


	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill
	Mapping based approaches
	\begin{itemize}
	\item Learn monolingual embedding separately
	\begin{itemize}
		\item Skip-gram model 
	\end{itemize}
	\item Learn linear mapping between embedding spaces	
	\begin{itemize}
		\item Supervised learning
		\begin{itemize}
			\item Procrustes analysis
		\end{itemize}
		\item Semi-supervised learning
		\begin{itemize}
			\item Iterative self-learning framework 
		\end{itemize}
		\item Unsupervised learning
		\begin{itemize}
			\item Adversarial learning
		\end{itemize}
	\end{itemize}
	\item Synthetic dictionary induction
	\begin{itemize}
		\item Nearest neighbor search
	\end{itemize}
	
\end{itemize}
	\vfill
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Monolingual Embedding}
	\vfill



	\texttt{Fasttext} \cite{bojanowski2017enriching}
	\begin{itemize}
		\item Essentially an extension of skip-gram/CBOW model
		\item Treat each word as compound of character ${n}$-gram
		\item Learn the internal structure of words
		\item Score function between context word $c$ and current word $w$
		 \[s(w, c) = \sum_{g \in G_{w}} z_g^{T} c \]
		 \begin{itemize}
		 \item  ${G_w}$: set of $n$-gram appears in ${w}$ 
		 \item $z_g$,${v_c}$: the corresponding  embedding
		 \end{itemize}
	\end{itemize}
	
	
	\vfill	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Supervised Learning}
	\vfill
	Assume given
	\begin{itemize}
		\item Word embedding \\
		trained  independently for each language on monolingual corpora
		\item Bilingual dictionary \\
		a known dictionary with pairs of words ${ \{ \bm{f}, \bm{e} \} }$ size $N$ 
	\end{itemize}
	
	
	
	Learn a linear mapping ${W \in \mathbb{R}^{d \times d}}$ such that
	\[W^*= \argmin\limits_{W \in \mathbb{R}^{d \times d}} {\sum_{n=1}^{N} {\lVert Wf_i -e_i \rVert}^2} \]
	\begin{itemize}
		\item ${d:}$ Dimension of embedding
		\item $f_n, e_n \in \mathbb{R}^d $: the embedding pair of corresponding word pair in the dictionary
	\end{itemize}
	\vfill
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Procrustes Analysis}
	\vfill
	Constrain ${W}$ to be an orthogonal matrix
	\begin{itemize}
		\item Enforce monolingual invariance
		
		\item Simplify the problem as the Procrustes problem
		
		\begin{itemize}
			\item A closed-form solution obtained from SVD
			\item $E$, $F \in \mathbb{R}^{d*N}$ denotes embedding projection of word pairs ${\{e,f\}}$
		\end{itemize}
		
		\[ W^* = \argmin\limits_{W \in \mathbb{R}^{d \times d}} {\lVert WF-E\lVert}_F^2  =  UV^T\]
		\[ U\varSigma V^T =  \textrm{SVD}(EF^T)\]
		
		\item Can be efficiently computed in linear time  w.r.t. seed dictionary size $N$
	\end{itemize}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\NewPage
	\headline{Semi-supervised Learning}
	\vfill
	Problem
	\begin{itemize}
		\item Large dictionary not readily available for many language pairs\\
	\end{itemize}
	Self-learning framework
	\begin{enumerate}
		\item Given source and target embedding ${\mathcal{F}}$ ${\mathcal{E}}$ , seed dictionary $D$
		\item Learn mapping with dictionary
		\item Induce dictionary according to mapping
		\item Repeat step $2$, $3$ until converges\\
	\end{enumerate}
	
	Performance
	\begin{itemize}
		\item Model works with initial dictionary		
		\item Achieve comparable accuracy as supervised method
		\item Stuck in a poor local optimum without initial dictionary
	\end{itemize}
	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Unsupervised Learning}	
	\vfill

		
	
	Methods
	\begin{itemize}
		\item 	Learn bilingual embeddings without any bilingual evidence (this work)
		\begin{itemize}
			\item Adversarial training	\cite{conneau2017word}
		\end{itemize}
		\item Design the seed dictionary
		\begin{itemize}
			\item Shared words, digits and cognates \cite{artetxe2017learning}
			\item Design heuristics to 
			build the seed dictionary \\ \cite{hoshen2018iterative} \cite{artetxe2018robust}
		\end{itemize}
		
	\end{itemize}
	\vfill
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Adversarial Training}
	\vfill
	Model
	\begin{itemize}
		\item  ${\mathcal{F} = \left\{ f_1, \ldots f_{V_f} \right\} }$ and  ${\mathcal{E} = \left\{ e_1, \ldots e_{V_e} \right\} }$: set of embeddings, not parallel
		\item Discriminator is trained to discriminate ${W{f_n}}$ and ${e_n}$ with ${f_n}$, ${e_n}$ randomly sampled from ${ \mathcal{F},  \mathcal{E} }$ 
		\item Generator ${W}$ is trained to prevent the discriminator from making accurate prediction
		
	\end{itemize}
	
	
	
	Discriminator loss
	\[ \mathcal{L}_D(\theta_D|W) = -\frac{1}{N}\sum_{n=1}^{N}\log P_{\theta_D}(source=1| W f_n) - \frac{1}{M}\sum_{m=1}^{M}\log P_{\theta_D}(source=0| e_m)\]
	
	
	Generator  loss 
	\[ \mathcal{L}_D(W|\theta_D) = -\frac{1}{N}\sum_{n=1}^{N}\log P_{\theta_D}(source=0| W f_n) - \frac{1}{M}\sum_{m=1}^{M}\log P_{\theta_D}(source=1| e_m)\]
	



	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Dictionary Induction}
	\vfill
	Nearest neighbor search
	\begin{itemize}
		\item Hubness problem: some points (hubs) tends to be nearest neighbors of many points in high-dimensional space\\
	\end{itemize}
	Cross-domain Similarity Local Scaling (CSLS)
	\begin{itemize}

		\item Penalize the similarity score of hubs
			\begin{itemize}
				\item $N_T(Wf):$ target neighbours for mapped source embedding
				\item $r_T(W f):$ penalty for hubness
			\end{itemize}
	\end{itemize}
	\[r_T(W f) = \frac{1}{K} \sum_{e \in N_T(Wf)}\textrm{cos}(Wf, e) \]
	\[ \textrm{CSLS}(Wf, e) = 2\ \textrm{cos}(Wf, e)-r_{T}(Wf)-r_{S}(e)\]\\
	Bidirection dictionary induction
	\begin{itemize}
		\item Unidirectional dictionary might lead to local optima
		\item Include only the mutual nearest neighbors 
	\end{itemize}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Sentence Translation}
	\vfill
	Context-aware Beam Search
	\begin{itemize}
		\item Language model\\
	\end{itemize}
	Denoising Autoencoder
	\begin{itemize}
		\item Insertion
		\item Deletion
		\item Reordering
	\end{itemize}	
	\vfill
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	
	\headline{Context-aware beam search}
	\vfill
	Given a history $h$ of target word before $e$, the score of $e$ to be the translation of $f$:
	\[ L(e;f,h)=\lambda_{emb}\log q(f,e) + \lambda_{LM} \log p(e|h)\]	
	\begin{itemize}
		\item Lexicon score $q(f,e) \in [0,1] $ defined as:
		\[ q(f,e)= \frac{d(f,e)+1}{2}\]
		where $d(f,e)\in [-1,1]$ cosine similarity between $f$ and $e$. In experiments, lexicon score from linear scaling works better than others, e.g. sigmoid or softmax
		\item Empirically set ${\lambda_{emb}}$  as 1, ${\lambda_{LM}}$ as 0.1
	\end{itemize}	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Denoising}	
\vfill
	Basic idea
	\begin{itemize}
		\item Model ${noise}(e_1^I)$ by injecting artificial noise into clean sentences $e_1^I$
		\item Neural network learns to restore more smooth sentence from word-by-word translation\\
	\end{itemize}
	 Training criterion
	\[ L = E_{e_1^I \in E}[-log(e_1^I|noise(e_1^I))]\]
	\begin{itemize}
		\item $E$ denotes target corpus.
		\item In Seq2Seq training, $e_1^I$ as label, $noise(e_1^I)$  as input 
		\item Artificial noise:
		\begin{itemize}
			\item insertion, deletion, reordering
		\end{itemize} 
	\end{itemize}


\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Insertion}	
\vfill	
Insertion
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item Word-by-word translation always outputs a target word for every position
		\item Some common words are considered as redundant ones
	\end{itemize}
	\item Method
	\begin{itemize}
		\item For each position in a sentence, insert a frequent word according from set ${v_{ins}}$ to a probability distribution ${p_{ins}}$
		\item Denoising network learns to delete the word when translating
	\end{itemize}
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{insertion}
	\end{center}\vspace{0.5em}	
\end{itemize}

\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Deletion}
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item In contrary case: some words are not related to any source word
	\end{itemize}
	
	\item Realization
	\begin{itemize}
		\item For each position in a sentence, delete the word according to a probability distribution ${p_{del}}$ as input
		\item  Denoising network learns to add some potential words when translating
	\end{itemize}	
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{deletion}
	\end{center}\vspace{0.5em}	
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{Reordering}	
\begin{itemize}
	\item Motivation
	\begin{itemize}
		\item Generated words are not in a correct sequence of the target language
	\end{itemize}
	\item Method
	\begin{itemize}
		\item For each position of a sentence, swap the words within a limited distance $d_{per}$ as input
		\item Denoising network learns reordering information when translating
	\end{itemize}
	\begin{center}
		\vspace{0.5em}
		\hspace{-1cm}\includegraphics[width=0.8\linewidth]{denoising}
	\end{center}\vspace{0.5em}
\end{itemize}	



	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Experiment Settings }	
	\vfill
	\begin{itemize}
	\item Word embedding and LM trained on \texttt{News Crawl 2014-2017} (100M)
	\item BLEU evaluated on German$\leftrightarrow$English \texttt{newstest2016}
	\item {Word accuracy evaluated on  dictionaries released by Facebook}
		\begin{itemize}
			\item Dictionary built with internal translation tool
			\item Each word has 1-4 word translation(s)
			\item Top-1 accuracy			
		\end{itemize}
	\item Context-aware beam search
	\begin{itemize}
		\item Lexicon candidates: 100
		\item Beam width: 10
	\end{itemize}

	 
	\end{itemize}
	\vfill
%%%%%%%%%%%%%%%
\NewPage
\headline{Experiments}
\vfill

\begin{table}
	\setcounter{table}{1}
	\caption{Translation results on German$\leftrightarrow$English \texttt{newstest2016} and French$\leftrightarrow$English \texttt{newstest2014}.}
	\centering
	\begin{tabular}{>{\bfseries}l>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c}
		\toprule
		& OOV & de-en & en-de & fr-en & en-fr\\
		System & handling  & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%]\\
		\midrule
		Word-by-Word & None  & 11.1 & 6.7 & 10.6 & 7.8\\
		\midrule
		+ LM (5-gram)  & LM  & 12.9 & 8.9 & 12.7 & 10.0\\
		&  Copy	& 14.5 & 9.9 & 13.6 & 10.9\\
		\midrule
		\hspace{10pt}+ Denoising (RNN) &  & 16.2 & 10.6 & 15.8 & 13.3 \\
		\hspace{10pt}+ Denoising (Transformer) & & \leavevmode\color{blue}{17.2} & \leavevmode\color{blue}{11.0} & \leavevmode\color{blue}{16.5} & \leavevmode\color{blue}13.9 \\
		\midrule
		\cite{lample2017unsupervised} & & 13.3 & 9.6 & 14.3 & 15.1\\
		\cite{artetxe2017unsupervised} & & - & - & 15.6 & 15.1\\
		\bottomrule
	\end{tabular}
	
	\label{tab:results}
\end{table}	
\vfill	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Ablation studies}	
	\vfill
	\begin{itemize}
		\item Different sizes of training corpora
		\item Different vocabularies: BPE and word
		\item Different vocabulary sizes for cross-lingual training
		\item Different denoising parameters
		\item Phrase embedding
		\item Vocabulary cut-off
		
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Different Training Corpora }	
	\vfill

	\begin{table}[!h]
		\centering
		\caption {Word-by-word translation from German to English}
		\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c}
			\hline
			&\textsc{Accuracy} [\%]& \textsc{Bleu} [\%] \\ \hline
			5M & 44.9  & 9.7  \\ \hline
			10M & 51.6 & 10.1 \\ \hline
			50M & 59.4 & 10.8 \\ \hline
			100M &\leavevmode\color{blue}61.2 & \leavevmode\color{blue}11.2 \\ \hline
		\end{tabular}
	
	\end{table}
	
	\begin{itemize}
		\item Larger corpus improves the word translation accuracy
		\item Also improves the word-by-word translation
	\end{itemize}
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Different Embeddings and Training Vocabulary Size}

	
	\begin{table}[!ht]
		\centering
		\scalebox{0.9}{
		\begin{tabular}{>{\bfseries}l>{\bfseries}c>{\bfseries}c}
			\toprule
			\multicolumn{2}{c}{\textbf{Vocabulary}} & \textsc{Bleu} [\%] \\
			\midrule
			& Merges \\
			\cmidrule{2-2}
			\multirow{3}{*}{BPE} & 
			20k & 10.4 \\
			& 50k & 12.5 \\
			& 100k & \leavevmode\color{blue}13.0 \\
			\midrule
			& Cross-lingual training \\
			\cmidrule{2-2}
			\multirow{4}{*}{Word} & 20k & 14.4\\
			& 50k & 14.4\\
			& 100k & \leavevmode\color{blue}14.5\\
			& 200k & 14.4\\
			\bottomrule
		\end{tabular}
			}
		

	\end{table}

	\begin{itemize}
		\item Word-by-word translation with language model
		\item Word embedding performs better than BPE embedding
		\item Embedding trained on 20k similar to 200k $\Rightarrow$ Frequent words matter
	\end{itemize}	
		\vfill





	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Denoising Experiments}
		\vfill

		\begin{table}[!h]
			\centering
			\scalebox{1}{
			\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}r>{\bfseries}c}
				\toprule
				$d_\text{per}$ & $p_\text{del}$ & $p_\text{ins}$ & $v_\text{ins}$ & \textsc{Bleu} [\%] \\
				\midrule
				2 & & & & 14.7\\
				3 & & & & \leavevmode\color{blue}{14.9}\\
				5 & & & & 14.9\\
				\midrule
				\multirow{2}{*}{3} & 0.1 & &  & \leavevmode\color{blue}{15.7} \\
				& 0.3 & & & 15.1 \\
				\midrule
				\multirow{4}{*}{3} & \multirow{4}{*}{0.1} & \multirow{4}{*}{0.1} & 10 & 16.8 \\
				& & & 50 & \leavevmode\color{blue}{17.2} \\
				& & & 500 & 16.8 \\
				& & & 5000 & 16.5\\
				\bottomrule
			\end{tabular}
			}
			\setcounter{table}{1}
			\label{tab:denoising}
		\end{table}

	\begin{itemize}
		\item Each artificial noise improves the translation performance
	\end{itemize}
		\vfill	
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%in a small range%%%%%%%%%%%%%%%%%
\NewPage
\headline{Phrase Embedding}
Motivation
\begin{itemize}
	\item Many phrases have a meaning that is not a simple composition of the meaning of its individual words\\
\end{itemize}
Phrase detection
\begin{itemize}
	\item Phrases formed based on the unigram and bigram counts:\\
	\cite{mikolov2013distributed} 
	\begin{itemize}
		\item Tune a good threshold value for score
	\end{itemize}
	\[ \textrm{score}(e^\prime, e) = \frac{\textrm{count}(e^\prime, e) - \delta}{\textrm{count}(e^\prime)*\textrm{count}(e)} \]	
	\item Process sentences with most common phrases in training corpus
	\begin{itemize}
		\item Count the most frequent bi-gram phrases: ${\textrm{score}(e^\prime, e) = \textrm{count}(e^\prime, e)}$
		\item Detect phrases as top frequent phrases in the training corpus
	\end{itemize}
	
\end{itemize}	
\NewPage
\headline{Phrase Embedding Experiments}
\vfill
\begin{table}[]
	\centering
	\begin{tabular}{>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c>{\bfseries}c  >{\bfseries}c}
		\hline
		\multicolumn{3}{c}{\multirow{2}{*}{\textbf{Vocabulary}}}                  & No LM & With LM & Denoising \\
		\multicolumn{3}{c}{}                                         &  \textsc{Bleu} [\%]  &  \textsc{Bleu} [\%] & \textsc{Bleu} [\%]   \\ \hline
		Word            & \multicolumn{2}{l}{}              & 11.2 & 14.5  &\leavevmode\color{blue}{ 17.2} \\
		 \hline
		\multirow{3}{*}{\cite{mikolov2013distributed} } & \multirow{3}{*}{threshold} & 100  & 11.1 & 13.7  & 15.6 \\ \cline{3-6} 
		&                            & 500  & 11.0 & 13.7  & 16.2 \\ \cline{3-6} 
		&                            & 2000 & 10.7 & 14.0  &16.5 \\ \hline
		Top frequent              & \multicolumn{1}{l}{\textbf{count}}  & 50k  & \leavevmode\color{blue}12.0 & \leavevmode\color{blue}15.7  & 16.8 \\ \hline
	\end{tabular}
\end{table}
\begin{itemize}
	\item Phrase embeddings helps only for WBW and  $+$LM
\end{itemize}
\vfill

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{	Source and Target Vocabulary Cut-off}

\begin{itemize}
	\item Limit the vocabulary size,  copy the OOV directly (mainly name entities) 
	\item Column: source vocabulary size/ row: target vocabulary size
\end{itemize}


\begin{table}
	\parbox{.5\linewidth}{
		\centering
		\caption{Word embedding vocabulary cut-off}
		\begin{tabular}{>{\bfseries}c >{\bfseries}c >{\bfseries}c >{\bfseries}c } 
			\hline
			\textsc{Bleu} [\%]	& 20k & 50k & 100k \\
			\hline
			50k &	11.1  & \leavevmode\color{blue}11.3 & 11.2  \\ 
			\hline
			100k&	11.2  & 11.2 & 11.1 \\ 			
			\hline
			150k&	10.9 & 10.9 & - \\
			\hline
		\end{tabular}
		
	}
	\hfill
	\parbox{.5\linewidth}{
		\centering
		\caption{Phrase embedding vocabulary cut-off}
		\begin{tabular}{>{\bfseries}c >{\bfseries}c >{\bfseries}c >{\bfseries}c } 
			\hline
			\textsc{Bleu} [\%]	& 50k & 100k & 150k \\
			\hline
			50k &	11.3  & - & -  \\ 
			\hline
			100k&	11.9  & 11.9 & - \\ 			
			\hline
			150k&	\leavevmode\color{blue}12.0 & 11.9 & 11.9 \\
			\hline
			200k & 12.0 & - & - \\
			\hline
		\end{tabular}
		
	}
\end{table}

\begin{itemize}
	\item Vocabulary size affects the translation performance slightly
\end{itemize}



\vfill	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\NewPage
\headline{LM Supported Cross-lingual Embedding training}
\vfill
Motivation
\begin{itemize}
	\item Language model help to select candidates, provide better dictionary
	\item Straightforward modeling, larger dictionary, different mapping types and loss functions\\
\end{itemize}
Model
\begin{itemize}
	\item Training the mapping with SGD instead of Procrustes analysis
	\item Dictionary from the sentence translation with LM, instead of induction
		


\end{itemize}

\begin{minipage}[b]{0.5\textwidth}
			\[ 
	\left. \begin{array}{c c} 
	(f_1,& e_1)\\
	(f_2,& e_2)\\
	\multicolumn{2}{c}{\vdots}\\
	(f_N,& e_N)
	\end{array} \right\} 
	\Rightarrow D
	\]
	
\end{minipage}
\begin{minipage}[b]{0.3\textwidth}
\[\mathcal{L} = \sum_{(f,e)\in D} {\lVert Wf - e \rVert}^2  \]
\end{minipage}

\vfill

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Conclusions}
	\vfill
	Comprehensive results
	\begin{itemize}
		\item Context-aware beam search with LM helps the lexicon choice
		\item Denoising networks aimed at insertion/deletion/reordering noise works for such problems in a small range of sentences\\
	\end{itemize}
	Ablation studies
	\begin{itemize}
		\item BPE embeddings performs worse than word embeddings, especially with smaller
		vocabulary size.
		\item Word-by-word translation with cross-lingual
embedding depends highly on the frequent word mappings
		\item Phrase embedding only helps in word-by-word translation with LM
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Outlook}
	\vfill
	Goal: Improve the unsupervised learning for cross-lingual embedding

	\begin{itemize}
		\item Accordingly improves unsupervised MT performance
		\item Other applications: transfer learning for low-resource LM \cite{adams2017cross}\\
	\end{itemize}
   

	 Non-linear mapping between source and target
	 \begin{itemize}
	 	\item Linear assumption may be too crude
	 	\item Stochastic gradient descent instead of SVD
	 	\item Also applies in supervised case
	 \end{itemize}
	\vfill

	
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\FinalPage
	

	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\bibliographystyle{i6bibliostyle}
	\bibliography{references}
	
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
