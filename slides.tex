%%
%% i6-style slides
%% customized for kim
%%

\documentclass[11pt, a4paper, landscape]{article}
\usepackage[userlastpage,triangle,utf-8,noblackslide]{NeyDreuwSlides_Oct08}
\usepackage{snapshot}
\usepackage{mathtools}
\usepackage{amssymb,amsmath,amsthm,enumitem}
\usepackage[caption]{subfig}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{bm}
\usepackage{enumerate}

% slides tunning
% \setlength{\footskip}{0pt} % default ist +30pt - negative Werte nicht moeglich
% \setlength{\headsep}{-10pt} % default ist +25pt - negative Werte moeglich
% \addtolength{\topmargin}{-5mm} % bewegt den gesamten Inhalt der Seite nach oben
% \setlength{\textheight}{170mm} % wenn moeglich nicht veraendern, beinflusst den Inhalt der Seiten und somit auch die Anzahl der Seiten!

% math operators and
% symbols %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\nat}{\ensuremath{\rm I\!N}\xspace}
\newcommand*{\rel}{\ensuremath{\rm I\!R}\xspace}
\newcommand*{\argmin}{\ensuremath{\operatornamewithlimits{argmin}}\xspace}
\newcommand*{\argmax}{\ensuremath{\operatornamewithlimits{argmax}}\xspace}
\newcommand*{\congmod}{\ensuremath{\operatornamewithlimits{\cong}}\xspace}
\newcommand*{\invers}{\ensuremath{\frac{1}}\xspace}
\newcommand*{\ra}{\ensuremath{\Rightarrow}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% custom packages
\usepackage{fancyvrb} %%% fancy verbatim to enable coloring within verbatim environments
\usepackage{bbding}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{tikz}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\renewcommand{\arraystretch}{1.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% main title of the work (used for \TitlePage)
\renewcommand*{\title}{Unsupervised Learning of Neural Network Lexicon\\ and Cross-lingual Word Embedding}
% short title (used for \lfoot)
\renewcommand*{\titleshort}{Unsupervised ANN Lexicon and Cross-lingual Embedding}
% (used for \TitlePage)
\renewcommand*{\occasion}{Master Thesis Proposal\\}
% short occasion title (used for \rfoot)
\renewcommand*{\occasionshort}{}
% default is \today (used for \TitlePage and \rfoot)
\renewcommand*{\date}{\today}
% all the authors of the work, can be long (used for \TitlePage)

\renewcommand*{\author}{Jiahui Geng}

% all the authors of the work, should be short (used for \lfoot)
\renewcommand*{\authorshort}{J. Geng:\xspace}
% all email address(es) of the authors (used for \TitlePage)
\renewcommand*{\email}{\url{jiahui.geng@rwth-aachen.de}}
% the author(s) who presented the work (used for \TitlePage)
\renewcommand*{\mainauthor}{Jiahui Geng}
% presenter mail address(es) (used for \FinalPage)
\renewcommand*{\mainauthoremail}{\url{jgeng@cs.rwth-aachen.de}}
% web address (used for \TitlePage _and_ \FinalPage)
\renewcommand*{\www}{http://www.hltpr.rwth-aachen.de/}
% keywords, can be used for PDF summary
\newcommand*{\keywords}{}

% will be set into the PDF document summary
\hypersetup{ pdftitle={\title}, pdfsubject={\occasion},
	pdfauthor={\author}, pdfkeywords={\keywords}, pdfpageduration = 2,
	pdfpagetransition = {Box /M /O /D 1}, }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listfiles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	%\setlength{\abovedisplayskip}{5pt}
	%\setlength{\belowdisplayskip}{3pt}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\TitlePage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
	\NewPage
	
	\headline {Outline}
	
	\vfill
	\begin{description}
		\item Introduction
		\item Literature
		\item Unsupervised word-by-word translation system
			\begin{itemize}
				\item Model
				\item Word translation
					\begin{itemize}
						\item Lonolingual word embedding
						\item Linear mapping between embedding space
						 
					\end{itemize}
				\item Sentence Translation
				\item Experiments
			\end{itemize}
		\item Outlook
	\end{description}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Introduction}
	
	\vfill
	\begin{description}
		\item Why unsupervised learning?
		\begin{itemize}
			\item Overcome the lack of reference translations
			\item Rich monolingual sentence resource \\
		\end{itemize}

		
	\end{description}
	\vfill
	

	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised cross-lingual embedding
	\begin{itemize}
		\item 	\cite{artetxe2017learning} Learning bilingual word embeddings with (almost) no bilingual data
		\begin{itemize}
			\item A self-learning framework combining embedding mapping and dictionary induction techniques, need small dataset to start
		\end{itemize}	    
		\item 	\cite{conneau2017word} Word translation without parallel data
		\begin{itemize}
			\item Implementation of GANs: the discriminator plays an adversarial role to a generative model and is trained to distinguish between two distributions
		\end{itemize}	    
		\item \cite{hoshen2018iterative} An Iterative Closest Point Method for Unsupervised Word Translation
		\begin{itemize}
			\item Iterative Closest Point Method for unsupervised embedding mapping, fewer hyper-parameters, more interpretable
		\end{itemize}	    
	\end{itemize}
	
	
	
	
	
	
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Literature}
	
	\vfill
	Unsupervised machine translation
	\begin{itemize}
		\item 	\cite{artetxe2017unsupervised} Unsupervised neural machine translation
		\begin{itemize}
			\item With fixed cross-lingual embeddings to train a shared encoder, train the system with de-noising and on-the-fly back-translation alternatively
		\end{itemize}
		\item \cite{lample2017unsupervised} Unsupervised Machine Translation Using Monolingual Corpora Only
		\begin{itemize} 
			\item Seq2seq model with encoder and decoder for both language, also with denoise autoencoder and back-translation
		\end{itemize}
		
		\item \cite{artetxe2017unsupervised} Phrase-Based \& Neural Unsupervised Machine Translation
		\begin{itemize}
			\item Simplifying the architecture and loss function while still following the above mentioned principles and propose a PBSMT with back-translation
		\end{itemize}	
		

	\end{itemize}
	
	\vfill


	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill

	Definition
	\begin{itemize}
		\item Word embedding of multiple languages in a joint embedding space		\\
	\end{itemize}
	Roles in unsupervised machine translation 
	\begin{itemize}
		\item Initilization
			\begin{itemize}
				\item Populate the initial phrase tables using scores from cross-lingual embeddings
				\item Preserve some of the original semantics
			\end{itemize}
		\item Shared Latent representations
			\begin{itemize}
				\item Shared encoder for producing a language independent representation
				\item Enable back-translation further improvement of results
			\end{itemize}
		
	\end{itemize}
	\vfill

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Word Translation}
	\vfill
	\begin{itemize}
		\item Learn monolingual embedding separately
			\begin{itemize}
				\item Fasttext 
			\end{itemize}
		\item Learn linear mapping between embedding spaces	
			\begin{itemize}
				\item Supervised learning
					\begin{itemize}
						\item Procrustes analysis
					\end{itemize}
				\item Unsupervised learning
					\begin{itemize}
						\item Adversarial learning
						\item Iterative refinement
					\end{itemize}
			\end{itemize}
		\item Bidirectional dictionary induction
			\begin{itemize}
				\item CSLS retrieval 
			\end{itemize}
	\end{itemize}
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Cross-lingual Word Embedding}
	\vfill

	

	
	
	
	
	Training Methods
	\begin{itemize}
		\item Mapping-based approaches
		\begin{itemize}
			\item Train word embeddings then learn mapping with bilingual dictionaries
		\end{itemize}
		\item Pseudo-multi-lingual corpora-based approaches
		\begin{itemize}
			\item Use monolingual word embedding methods on mixed corpus of multiple languages 
		\end{itemize}
		\item Joint methods
		\begin{itemize}
			\item Minimize the monolingual losses with the cross-lingual regularization term
		\end{itemize}
	\end{itemize}
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Supervised Learning}
	\vfill
	Assume we have
	\begin{itemize}
		\item Word embeddings \\
		trained  independently for each language on monolingual corpora
		\item Bilingual dictionary \\
		a known dictionary with pairs of words ${ \left\{ f, e \right\} }$
	\end{itemize}
	
	
	
	Learn a linear mapping ${W}$ such that
	\[W^*= \argmin\limits_{W} \lVert WF-E\rVert \]
	\begin{itemize}
		\item ${d:}$ Dimension of embeddings
		\item ${F}$, ${E}$: Aligned ${d \times s }$ real matrices containing the embeddings of the words in the dictionary
		\item ${s}$: Seed dictionary size
	\end{itemize}
	\vfill
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Procrustes Analysis}
	\vfill
	Constrain ${W}$ to be an orthogonal matrix
	\begin{itemize}
		\item Enforce monolingual invariance
		
		\item Simplify the problem as the the Procrustes problem\\ which has a closed-form solution obtained from the SVD of  ${EF^T}$:
		
		
		
		\[ W^* = \argmin\limits_{W \in M_d( \mathbb{R})} \lVert WF-E\lVert  =  UV^T\]
		\[ U\varSigma V^T =  SVD(EF^T)\]
		
		\item Can be efficiently computed in linear time ${w.r.t}$ number of dictionary entries
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Unsupervised Word Embedding Mapping}	
	\vfill
	Problem
	\begin{itemize}
		\item Large dictionary not readily available for many language pairs\\
	\end{itemize}
	
	
	Methods
	\begin{itemize}
		\item Design the seed dictionary
		\begin{itemize}
			\item Using document-aligned corpora to extract the training dictionary
			\item Relying on shared words, digits and cognates
		\end{itemize}
		\item 	Learn bilingual embeddings without any bilingual evidence
		\begin{itemize}
			\item Adversarial training
			
		\end{itemize}
	\end{itemize}
	\vfill
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Adversarial Training}
	\vfill
	Model
	\begin{itemize}
		\item  ${\mathcal{F} = \left\{ f_1, \ldots f_{V_f} \right\} }$ and  ${\mathcal{E} = \left\{ e_1, \ldots e_{V_e} \right\} }$: Sets of word embeddings
		\item Discriminator is trained to discriminate between elements randomly sampled from ${W \mathcal{F}}$ and ${\mathcal{E}}$ 
		\item Generator ${W}$ is trained to prevent the discriminator from making accurate prediction
		
	\end{itemize}
	
	
	
	Discriminator loss
	\[ \mathcal{L}_D = -\log D(\mathcal{E}) - \log (1-D(W\mathcal{F}))\]
	
	
	Generator  loss 
	\[ \mathcal{L}_W = -\log D(W\mathcal{F})\]
	\vfill
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Trick: Frequency-based Vocabulary Cutoff}	
	\vfill
	Problem
	\begin{itemize}
		\item Rare word embeddings are less trained(updated)
		\item Contain noise information for alignment
	\end{itemize}
	Experiment
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\NewPage
	\headline{Iterative Refinement}
	\begin{figure}[ht]
		\centering
		\begin{minipage}{.7\linewidth}
			\begin{algorithm}[H]
				\SetAlgoLined
				\KwIn{$\mathcal{F}$ (source embeddings)}
				\KwIn{$\mathcal{E}$ (target embeddings)}
				\KwIn{$\mathcal{D}$ (seed dictionary)}
				\KwResult{$\mathcal{W}$ (embedding mapping) }
				initialization\;
				\While{not convergence criterion}{
					 $\mathcal{W} \leftarrow  \bm{learn\_mapping} \mathcal{(E, F, D)}$\;
					 $\mathcal{D} \leftarrow \bm{learn\_mapping}\mathcal{(E, F, W)}$ \;
				}
			
			\caption{Self-learning framework}
			\end{algorithm}
		\end{minipage}
	\end{figure}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Dictionay Induction}

	\vfill
	Cross-domain Similarity Local Scaling
	\begin{itemize}
		\item KNN suffers from the hubness problem
		\item Penalize the similarity score of hubs
			\begin{itemize}
				\item $N_T(Wf):$ target neighbours for mapped source embedding
				\item $r_T(W f):$ penalty for hubness
			\end{itemize}
	\end{itemize}
	\[r_T(W f) = \frac{1}{K} \sum_{e \in N_T(Wf)}cos(Wf, e) \]
	\[ CSLS(Wf, e) = 2cos(Wf, e)-r_{T}(Wf)-r_{S}(e)\]
	Bidirection Dictionary Induction

	\begin{itemize}
		\item Repreated word in unidirectional dictionary might lead to local optima
		\item Include the dictionary in both directions
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Sentence Translation}
	\vfill
	Context-aware beam search
	\begin{itemize}
		\item Given a history $h$ of target word before $e$, the score of $e$ to be the translation of $f$:
		\[ L(e;f,h)=\lambda_{emb}q(f,e) + \lambda_{LM}p(e|h)\]
		\item Lexicon score $q(f,e) \in [0,1] $ defined as:
		\[ q(f,e)= \frac{d(f,e)+1}{2}\]
		\item $q(f,e)\in [-1,1]$ cosine similarity between $f$ and $e$
		\item In experiments, such lexicon score works better than others, e.g. sigmoid or softmax
	\end{itemize}
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Denoising}	
	\vfill
	
	
	\begin{itemize}
		\item Substitions, insertions, deletions, reordering viewed as noise in word-by-word translation
		\item Model such noise $c(t)$ by injecting artificial noise into clean sentences $t$
		\item Language modelling via denoising autoencoder can improve the translation by minimizing:
		\[ L = E_{t \in T}[-logP_{t\rightarrow t}(t|C(t))]\]
		\item In Seq2Seq training, $t$ as label, $c(t)$  as input 
	\end{itemize}
	
	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Results}
	\vfill
	
	\begin{table}
	\setcounter{table}{1}
	\caption{Translation results on German$\leftrightarrow$English \texttt{newstest2016} and French$\leftrightarrow$English \texttt{newstest2014}.}
	  \centering
		\begin{tabular}{lcccc}
		  	\toprule
		  	& de-en & en-de & fr-en & en-fr\\
		  	System & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%] & \textsc{Bleu} [\%]\\
		  	\midrule
		  	Word-by-Word  & 11.1 & 6.7 & 10.6 & 7.8\\
		  	\midrule
		  	+ LM  & 12.9 & 8.9 & 12.7 & 10.0\\
		  	& 14.5 & 9.9 & 13.6 & 10.9\\
		  	\midrule
		  	\hspace{10pt}+ Denoising (RNN)  & 16.2 & 10.6 & 15.8 & 13.3 \\
		  	\hspace{10pt}+ Denoising (Transformer) & \textbf{17.2} & \textbf{11.0} & \textbf{16.5} & 13.9 \\
		  	\midrule
		  	\cite{lample2017unsupervised} & 13.3 & 9.6 & 14.3 & 15.1\\
		  	\cite{artetxe2017unsupervised} & - & - & 15.6 & 15.1\\
		  	\bottomrule
		\end{tabular}

		\label{tab:results}
	\end{table}	
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline{Phrase Embedding}
	Motivation
	\begin{itemize}
		\item Many phrases have a meaning that is not a simple composition of the meaning of its individual words
	\end{itemize}
	Phrase detection
	\begin{itemize}
		\item Phrases formed based on the unigram and bigram counts
		\[ score(w_i, w_j) = \frac{count(w_i, w_j) - \delta}{count(w_i)*count(w_j)} \]
	\end{itemize}
	Translation performance
	\begin{itemize}
		\item Higher threshold, fewer phrases
		\item Topk: 500 beam size: 30
	\end{itemize}
	\begin{center}
		\begin{tabular}{|c|c|c|c| } 
			\hline
			threshold  & 100 & 500 & 2000 \\ 
			\hline
			BLEU  & 100 & 13.83 & 13.65 \\ 			
			\hline
		\end{tabular}
	\end{center}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {Outlook}
	\vfill
	Cross-lingual word embedding and word-to-word MT system
	\begin{itemize}
		\item Develop a new training algorithm for cross-lingual embeddings
		\begin{itemize}
			\item Context/domain considered, e.g. LM
			\item Better constraints on specific (group of) words\\
		\end{itemize}
		\item Word-to-word MT system with cross-lingual embeddings
		\begin{itemize}
			\item Efficient nearest neighbour search
			\item Combination with a language model\\
		\end{itemize}
		\item Compare translation results with word-to-word neural lexicons
		\begin{itemize}
			\item  All trained/tested on intact corpora without artificial change of alignments
			
		\end{itemize}
	\end{itemize}
	\vfill
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\headline {\appendixname: Denoising \& Vocabulary}
	\vfill
	\begin{table}[!ht]
		\centering
		\begin{tabular}{ccrc}
			\toprule
			$d_\text{per}$ & $p_\text{del}$ & $p_\text{ins}$ & \textsc{Bleu} [\%] \\
			\midrule
			2 & & & 14.7\\
			3 & & & \textbf{14.9}\\
			5 & & & 14.9\\
			\midrule
			\multirow{2}{*}{3} & 0.1 & & \textbf{15.7} \\
			& 0.3 & & 15.1 \\
			\midrule
			\multirow{6}{*}{3} & \multirow{6}{*}{0.1} & 10 & 16.8 \\
			& & 50 & \textbf{17.2} \\
			& & 500 & 16.8 \\
			& & 5000 & 16.5\\
			\bottomrule
		\end{tabular}
		\caption{Translation results with different values of denoising parameters.}
		\setcounter{table}{1}
		\label{tab:denoising}
	\end{table}
	\vfill
	

	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\vfill
	\begin{table}[!ht]
		\centering
		\begin{tabular}{lcc}
			\toprule
			\multicolumn{2}{c}{Vocabulary} & \textsc{Bleu} [\%] \\
			\midrule
			& Merges \\
			\cmidrule{2-2}
			\multirow{3}{*}{BPE} & 20k & 10.4 \\
			& 50k & 12.5 \\
			& 100k & 13.0 \\
			\midrule
			& Cross-lingual \\
			& Training \\
			\cmidrule{2-2}
			\multirow{4}{*}{Word} & 20k & 14.4\\
			& 50k & 14.4\\
			& 100k & 14.5\\
			& 200k & 14.4\\
			\bottomrule
		\end{tabular}
		\caption{Translation results with different vocabularies.}
		\label{tab:vocab}
	\end{table}
	\vfill

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\FinalPage
	

	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\NewPage
	\bibliographystyle{i6bibliostyle}
	\bibliography{references}
	
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
